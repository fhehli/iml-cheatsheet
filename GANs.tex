\section*{GANs}
Goal: Approximate a distribution $X\approx G_w(Z)$ where $Z$
is a simple distribution.
Train $G$ and $D$ simultaneously 
on the objective
$$\min_G\max_D\E_x[\log D(x)] +\E_Z[\log1-D(G(Z))].$$
Training requires finding a saddle point. If 
$G, D$ are complex enough, then the distribution of $X$
minimizes the objective. Cannot compute likelihood on holdout set.
If $D$ is globally optimal for some $G$, then $D$ predicts
$\frac{p_X(x)}{p_G(x)+p_X(x)}$.